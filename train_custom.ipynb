{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data.othello import Othello, OthelloBoardState, permit_reverse\n",
    "from mingpt.dataset import CharDataset\n",
    "from mingpt.utils import sample\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.trainer import Trainer, TrainerConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mem Used: 4.15 GB: 100%|██████████| 50/50 [00:15<00:00,  3.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4999952 from 50 files. Now deduplicating...\n",
      "Deduplicating finished with 4999389 games left\n",
      "Using 3999512 for training, 999877 for validation\n",
      "Dataset created has 3999512 sequences, 61 unique words.\n"
     ]
    }
   ],
   "source": [
    "# n_games=-1 means use as many simulated games as possible (from \"data/othello_synthetic/\")\n",
    "othello = Othello(n_games=-1)\n",
    "train_dataset = CharDataset(othello)\n",
    "# original OthelloGPT params: n_layer=8, n_head=8, n_embd=512\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    print(\"NO GPU FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_20230704_154558\n"
     ]
    }
   ],
   "source": [
    "# setting up training\n",
    "max_epochs = 40\n",
    "experiment_name = \"bias80\"\n",
    "t_start = time.strftime(\"_%Y%m%d_%H%M%S\")\n",
    "ckpt_path = f\"./ckpts/{experiment_name}_{t_start}.ckpt\"\n",
    "tconf = TrainerConfig(\n",
    "    max_epochs=max_epochs, \n",
    "    batch_size=512*4, # using 4 gpus\n",
    "    learning_rate=5e-4,\n",
    "    lr_decay=True, \n",
    "    warmup_tokens=len(train_dataset)*train_dataset.block_size*5, \n",
    "    final_tokens=len(train_dataset)*train_dataset.block_size*max_epochs,\n",
    "    num_workers=0, \n",
    "    ckpt_path=ckpt_path, \n",
    "    # saved_epochs=[0, 1, 2, 3, 5, 10, 15, 20],\n",
    ")\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "device = trainer.device\n",
    "print(t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model from ckpt\n",
    "# ckpt = \"grok/synth\"\n",
    "ckpt = \"bias80_e40\"\n",
    "load_res = model.load_state_dict(torch.load(f\"./ckpts/{ckpt}.ckpt\"))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    print(\"NO GPU FOUND\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if model prediction is legal for each node in given game\n",
    "# expects dataset has already been loaded and model is on GPU\n",
    "def check_legal(model, game):\n",
    "    total_nodes = 0\n",
    "    success_nodes = 0\n",
    "\n",
    "    len_whole_game = len(game)\n",
    "    for len_partial_game in range(1, len_whole_game):\n",
    "        total_nodes += 1\n",
    "        context = game[:len_partial_game]\n",
    "        x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None, ...].to(device)\n",
    "        y = sample(model, x, 1, temperature=1.0)\n",
    "        # taking top-1 prediction\n",
    "        completion = [train_dataset.itos[int(i)] for i in y[0] if i != -1]\n",
    "        try:\n",
    "            OthelloBoardState().update(completion)\n",
    "        except Exception:\n",
    "            pass\n",
    "        else:\n",
    "            success_nodes += 1\n",
    "    \n",
    "    return total_nodes, success_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default data root is othello_synthetic\n",
    "def validate_with_dataset(model, data_root=None, n_games=1000):\n",
    "    # find to load in first n games, because the first ~1 million othello_synthetic games are test set for unbiased model\n",
    "    v_games = Othello(data_root=data_root, n_games=n_games, test_split=1)\n",
    "\n",
    "    total_nodes = 0\n",
    "    success_nodes = 0\n",
    "\n",
    "    def progress_report():\n",
    "        return f\"{success_nodes/total_nodes*100:.2f}% pass rate: {success_nodes}/{total_nodes} among all searched nodes\"\n",
    "    \n",
    "    bar = tqdm(v_games.val[:n_games])\n",
    "    for game in bar:\n",
    "        tn, sn = check_legal(model, game)\n",
    "        total_nodes += tn\n",
    "        success_nodes += sn\n",
    "        bar.set_description(progress_report())\n",
    "    print(progress_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mem Used: 7.166 GB:   0%|          | 0/50 [00:00<?, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99999 from 1 files. Now deduplicating...\n",
      "Deduplicating finished with 50 games left\n",
      "Using 0 for training, 50 for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87.08% pass rate: 2569/2950 among all searched nodes: 100%|██████████| 50/50 [00:20<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.08% pass rate: 2569/2950 among all searched nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validate_with_dataset(model, data_root=\"othello_synthetic\", n_games=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_game = [19, 34, 41, 11, 10, 9, 1, 20, 3, 2, 8, 0, 13, 4, 29, 12, 5, 6, 14, 15, 21, 37, 22, 33, 7, 26, 18, 16, 25, 17, 42, 23, 30, 24, 31, 32, 46, 38]\n",
    "OthelloBoardState().update(partial_game, prt=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probe analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.944670627934276, 25.998496185446008, 26.326511150234744, 26.59829812206572, 26.910798122065728, 26.979478433098592, 27.150491490610328, 27.355670481220663\n"
     ]
    }
   ],
   "source": [
    "# summarizing probe loss\n",
    "\n",
    "import json\n",
    "\n",
    "root = f\"ckpts/grok/probes/state_tl256_random\"\n",
    "errs = []\n",
    "for i in range(8):\n",
    "    name = f\"layer{i+1}/tensorboard.txt\"\n",
    "    with open(f\"{root}/{name}\", \"r\") as file:\n",
    "        j = json.load(file)\n",
    "        test_acc = j['test_acc_cont']\n",
    "        err = 100 * (1 - test_acc[-1])\n",
    "        errs.append(err)\n",
    "        # print(f\"layer {i+1} error rate: {err:.5f}\")\n",
    "\n",
    "print(\", \".join([str(e) for e in errs]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
