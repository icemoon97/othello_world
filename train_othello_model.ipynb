{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data.othello import Othello, OthelloBoardState, permit_reverse\n",
    "from mingpt.dataset import CharDataset\n",
    "from mingpt.utils import sample\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.trainer import Trainer, TrainerConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_games=-1 means use as many simulated games as possible (from \"data/othello_synthetic/\")\n",
    "othello = Othello(n_games=-1, data_root=\"othello_synthetic\")\n",
    "train_dataset = CharDataset(othello)\n",
    "# original OthelloGPT params: n_layer=8, n_head=8, n_embd=512\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mem Used: 4.171 GB: 100%|██████████| 50/50 [00:17<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000000 from 50 files\n",
      "Deduplicating finished with 5000000 games left\n",
      "Using 4000000 for training, 1000000 for validation\n"
     ]
    }
   ],
   "source": [
    "othello = Othello(n_games=-1, data_root=\"othello_synthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created has 100 sequences, 61 unique words.\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "train_dataset = CharDataset(othello[:n])\n",
    "# original OthelloGPT params: n_layer=8, n_head=8, n_embd=512\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_res = model.load_state_dict(torch.load(f\"./ckpts/bias/TLbias80.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    print(\"NO GPU FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_20230718_132649\n"
     ]
    }
   ],
   "source": [
    "# setting up training\n",
    "max_epochs = 200\n",
    "experiment_name = \"ft_bias50_100\"\n",
    "t_start = time.strftime(\"_%Y%m%d_%H%M%S\")\n",
    "ckpt_path = f\"./ckpts/{experiment_name}_{t_start}.ckpt\"\n",
    "tconf = TrainerConfig(\n",
    "    max_epochs=max_epochs, \n",
    "    batch_size=512*4, # using 4 gpus\n",
    "    learning_rate=1e-4,\n",
    "    lr_decay=False,\n",
    "    # learning_rate=5e-4,\n",
    "    # lr_decay=True, \n",
    "    warmup_tokens=len(train_dataset)*train_dataset.block_size*5, \n",
    "    final_tokens=len(train_dataset)*train_dataset.block_size*max_epochs,\n",
    "    num_workers=0, \n",
    "    ckpt_path=ckpt_path, \n",
    "    saved_epochs=[1, 10, 50, 100, 200],\n",
    ")\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "device = trainer.device\n",
    "print(t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 0: train loss 3.27292. lr 1.000000e-04: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "epoch 2 iter 0: train loss 2.89988. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "epoch 3 iter 0: train loss 2.68424. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "epoch 4 iter 0: train loss 2.52461. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "epoch 5 iter 0: train loss 2.40875. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "epoch 6 iter 0: train loss 2.30894. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.60it/s]\n",
      "epoch 7 iter 0: train loss 2.21051. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "epoch 8 iter 0: train loss 2.13478. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "epoch 9 iter 0: train loss 2.05884. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
      "epoch 10 iter 0: train loss 1.98584. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s]\n",
      "epoch 11 iter 0: train loss 1.91992. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "epoch 12 iter 0: train loss 1.85974. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "epoch 13 iter 0: train loss 1.80697. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.76it/s]\n",
      "epoch 14 iter 0: train loss 1.74898. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.85it/s]\n",
      "epoch 15 iter 0: train loss 1.68240. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "epoch 16 iter 0: train loss 1.62475. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "epoch 17 iter 0: train loss 1.57813. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n",
      "epoch 18 iter 0: train loss 1.52433. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.93it/s]\n",
      "epoch 19 iter 0: train loss 1.47516. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s]\n",
      "epoch 20 iter 0: train loss 1.41378. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "epoch 21 iter 0: train loss 1.35298. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.82it/s]\n",
      "epoch 22 iter 0: train loss 1.31143. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.54it/s]\n",
      "epoch 23 iter 0: train loss 1.25827. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "epoch 24 iter 0: train loss 1.20481. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "epoch 25 iter 0: train loss 1.14917. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "epoch 26 iter 0: train loss 1.10387. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.69it/s]\n",
      "epoch 27 iter 0: train loss 1.03763. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.52it/s]\n",
      "epoch 28 iter 0: train loss 0.99300. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.62it/s]\n",
      "epoch 29 iter 0: train loss 0.95043. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "epoch 30 iter 0: train loss 0.91058. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "epoch 31 iter 0: train loss 0.86868. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "epoch 32 iter 0: train loss 0.80733. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.85it/s]\n",
      "epoch 33 iter 0: train loss 0.77357. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
      "epoch 34 iter 0: train loss 0.72420. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "epoch 35 iter 0: train loss 0.68697. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "epoch 36 iter 0: train loss 0.65129. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "epoch 37 iter 0: train loss 0.61152. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "epoch 38 iter 0: train loss 0.58903. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.41it/s]\n",
      "epoch 39 iter 0: train loss 0.53772. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "epoch 40 iter 0: train loss 0.50075. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "epoch 41 iter 0: train loss 0.48094. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.56it/s]\n",
      "epoch 42 iter 0: train loss 0.44952. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "epoch 43 iter 0: train loss 0.42873. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "epoch 44 iter 0: train loss 0.40050. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "epoch 45 iter 0: train loss 0.37600. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.78it/s]\n",
      "epoch 46 iter 0: train loss 0.36001. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s]\n",
      "epoch 47 iter 0: train loss 0.34032. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.64it/s]\n",
      "epoch 48 iter 0: train loss 0.31295. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "epoch 49 iter 0: train loss 0.29815. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "epoch 50 iter 0: train loss 0.28167. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.66it/s]\n",
      "epoch 51 iter 0: train loss 0.27167. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.60it/s]\n",
      "epoch 52 iter 0: train loss 0.25129. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "epoch 53 iter 0: train loss 0.24011. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s]\n",
      "epoch 54 iter 0: train loss 0.23169. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "epoch 55 iter 0: train loss 0.22614. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "epoch 56 iter 0: train loss 0.21191. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "epoch 57 iter 0: train loss 0.19925. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.85it/s]\n",
      "epoch 58 iter 0: train loss 0.19721. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "epoch 59 iter 0: train loss 0.18696. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.70it/s]\n",
      "epoch 60 iter 0: train loss 0.17987. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n",
      "epoch 61 iter 0: train loss 0.17292. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "epoch 62 iter 0: train loss 0.17189. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "epoch 63 iter 0: train loss 0.16124. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s]\n",
      "epoch 64 iter 0: train loss 0.15851. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]\n",
      "epoch 65 iter 0: train loss 0.15499. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "epoch 66 iter 0: train loss 0.14902. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.56it/s]\n",
      "epoch 67 iter 0: train loss 0.14379. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "epoch 68 iter 0: train loss 0.14550. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
      "epoch 69 iter 0: train loss 0.13973. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.66it/s]\n",
      "epoch 70 iter 0: train loss 0.13239. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "epoch 71 iter 0: train loss 0.12872. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "epoch 72 iter 0: train loss 0.13171. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
      "epoch 73 iter 0: train loss 0.12647. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n",
      "epoch 74 iter 0: train loss 0.12445. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.02it/s]\n",
      "epoch 75 iter 0: train loss 0.12123. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "epoch 76 iter 0: train loss 0.12303. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s]\n",
      "epoch 77 iter 0: train loss 0.11967. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.78it/s]\n",
      "epoch 78 iter 0: train loss 0.11930. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.20it/s]\n",
      "epoch 79 iter 0: train loss 0.11802. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "epoch 80 iter 0: train loss 0.11495. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.75it/s]\n",
      "epoch 81 iter 0: train loss 0.11571. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "epoch 82 iter 0: train loss 0.11446. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
      "epoch 83 iter 0: train loss 0.10985. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "epoch 84 iter 0: train loss 0.11259. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
      "epoch 85 iter 0: train loss 0.10978. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "epoch 86 iter 0: train loss 0.10827. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "epoch 87 iter 0: train loss 0.11035. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.82it/s]\n",
      "epoch 88 iter 0: train loss 0.10479. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.78it/s]\n",
      "epoch 89 iter 0: train loss 0.10414. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "epoch 90 iter 0: train loss 0.10556. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n",
      "epoch 91 iter 0: train loss 0.10510. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.27it/s]\n",
      "epoch 92 iter 0: train loss 0.10221. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.76it/s]\n",
      "epoch 93 iter 0: train loss 0.10030. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.28it/s]\n",
      "epoch 94 iter 0: train loss 0.10032. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.56it/s]\n",
      "epoch 95 iter 0: train loss 0.10419. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "epoch 96 iter 0: train loss 0.09921. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.73it/s]\n",
      "epoch 97 iter 0: train loss 0.10144. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.77it/s]\n",
      "epoch 98 iter 0: train loss 0.10071. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.95it/s]\n",
      "epoch 99 iter 0: train loss 0.10161. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.68it/s]\n",
      "epoch 100 iter 0: train loss 0.09562. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "epoch 101 iter 0: train loss 0.10056. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.78it/s]\n",
      "epoch 102 iter 0: train loss 0.09569. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n",
      "epoch 103 iter 0: train loss 0.09654. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "epoch 104 iter 0: train loss 0.09922. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s]\n",
      "epoch 105 iter 0: train loss 0.09699. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "epoch 106 iter 0: train loss 0.09667. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "epoch 107 iter 0: train loss 0.09424. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "epoch 108 iter 0: train loss 0.09456. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "epoch 109 iter 0: train loss 0.09321. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.66it/s]\n",
      "epoch 110 iter 0: train loss 0.09603. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "epoch 111 iter 0: train loss 0.09412. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "epoch 112 iter 0: train loss 0.09418. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.60it/s]\n",
      "epoch 113 iter 0: train loss 0.09338. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.83it/s]\n",
      "epoch 114 iter 0: train loss 0.09089. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s]\n",
      "epoch 115 iter 0: train loss 0.09154. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s]\n",
      "epoch 116 iter 0: train loss 0.09353. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.62it/s]\n",
      "epoch 117 iter 0: train loss 0.09177. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n",
      "epoch 118 iter 0: train loss 0.09281. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "epoch 119 iter 0: train loss 0.09101. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "epoch 120 iter 0: train loss 0.09074. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "epoch 121 iter 0: train loss 0.09104. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "epoch 122 iter 0: train loss 0.08999. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.64it/s]\n",
      "epoch 123 iter 0: train loss 0.08867. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "epoch 124 iter 0: train loss 0.08973. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "epoch 125 iter 0: train loss 0.08932. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "epoch 126 iter 0: train loss 0.09077. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "epoch 127 iter 0: train loss 0.09264. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.03it/s]\n",
      "epoch 128 iter 0: train loss 0.09067. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.97it/s]\n",
      "epoch 129 iter 0: train loss 0.09220. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.95it/s]\n",
      "epoch 130 iter 0: train loss 0.08956. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "epoch 131 iter 0: train loss 0.09032. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.76it/s]\n",
      "epoch 132 iter 0: train loss 0.08824. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.68it/s]\n",
      "epoch 133 iter 0: train loss 0.08879. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.21it/s]\n",
      "epoch 134 iter 0: train loss 0.08869. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "epoch 135 iter 0: train loss 0.08876. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n",
      "epoch 136 iter 0: train loss 0.09039. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\n",
      "epoch 137 iter 0: train loss 0.08729. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.74it/s]\n",
      "epoch 138 iter 0: train loss 0.08843. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.79it/s]\n",
      "epoch 139 iter 0: train loss 0.08767. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "epoch 140 iter 0: train loss 0.08769. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.66it/s]\n",
      "epoch 141 iter 0: train loss 0.08806. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "epoch 142 iter 0: train loss 0.08930. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.78it/s]\n",
      "epoch 143 iter 0: train loss 0.08841. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
      "epoch 144 iter 0: train loss 0.08950. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "epoch 145 iter 0: train loss 0.09013. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.46it/s]\n",
      "epoch 146 iter 0: train loss 0.08739. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.79it/s]\n",
      "epoch 147 iter 0: train loss 0.08601. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s]\n",
      "epoch 148 iter 0: train loss 0.08711. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.12it/s]\n",
      "epoch 149 iter 0: train loss 0.08699. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s]\n",
      "epoch 150 iter 0: train loss 0.08563. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.83it/s]\n",
      "epoch 151 iter 0: train loss 0.08731. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.29it/s]\n",
      "epoch 152 iter 0: train loss 0.08717. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "epoch 153 iter 0: train loss 0.08725. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "epoch 154 iter 0: train loss 0.08803. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s]\n",
      "epoch 155 iter 0: train loss 0.08766. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.68it/s]\n",
      "epoch 156 iter 0: train loss 0.08781. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.11it/s]\n",
      "epoch 157 iter 0: train loss 0.08560. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "epoch 158 iter 0: train loss 0.08614. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s]\n",
      "epoch 159 iter 0: train loss 0.08592. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.21it/s]\n",
      "epoch 160 iter 0: train loss 0.08334. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "epoch 161 iter 0: train loss 0.08580. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "epoch 162 iter 0: train loss 0.08380. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.10it/s]\n",
      "epoch 163 iter 0: train loss 0.08530. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "epoch 164 iter 0: train loss 0.08670. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "epoch 165 iter 0: train loss 0.08530. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.03it/s]\n",
      "epoch 166 iter 0: train loss 0.08571. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.82it/s]\n",
      "epoch 167 iter 0: train loss 0.08586. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "epoch 168 iter 0: train loss 0.08719. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.60it/s]\n",
      "epoch 169 iter 0: train loss 0.08513. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.65it/s]\n",
      "epoch 170 iter 0: train loss 0.08598. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.56it/s]\n",
      "epoch 171 iter 0: train loss 0.08679. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.51it/s]\n",
      "epoch 172 iter 0: train loss 0.08816. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "epoch 173 iter 0: train loss 0.08519. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.57it/s]\n",
      "epoch 174 iter 0: train loss 0.08520. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.68it/s]\n",
      "epoch 175 iter 0: train loss 0.08635. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "epoch 176 iter 0: train loss 0.08639. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "epoch 177 iter 0: train loss 0.08462. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n",
      "epoch 178 iter 0: train loss 0.08536. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.11it/s]\n",
      "epoch 179 iter 0: train loss 0.08514. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "epoch 180 iter 0: train loss 0.08714. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "epoch 181 iter 0: train loss 0.08549. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.65it/s]\n",
      "epoch 182 iter 0: train loss 0.08655. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
      "epoch 183 iter 0: train loss 0.08639. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s]\n",
      "epoch 184 iter 0: train loss 0.08552. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s]\n",
      "epoch 185 iter 0: train loss 0.08546. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n",
      "epoch 186 iter 0: train loss 0.08698. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.25it/s]\n",
      "epoch 187 iter 0: train loss 0.08515. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "epoch 188 iter 0: train loss 0.08692. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.83it/s]\n",
      "epoch 189 iter 0: train loss 0.08530. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "epoch 190 iter 0: train loss 0.08780. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "epoch 191 iter 0: train loss 0.08426. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "epoch 192 iter 0: train loss 0.08694. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]\n",
      "epoch 193 iter 0: train loss 0.08501. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "epoch 194 iter 0: train loss 0.08584. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
      "epoch 195 iter 0: train loss 0.08476. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "epoch 196 iter 0: train loss 0.08502. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.74it/s]\n",
      "epoch 197 iter 0: train loss 0.08548. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "epoch 198 iter 0: train loss 0.08424. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.49it/s]\n",
      "epoch 199 iter 0: train loss 0.08565. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 13.13it/s]\n",
      "epoch 200 iter 0: train loss 0.08562. lr 1.000000e-04: 100%|██████████| 1/1 [00:00<00:00, 12.57it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_othello_model(ckpt):\n",
    "    # original OthelloGPT params: n_layer=8, n_head=8, n_embd=512\n",
    "    # vocab_size = 59, block_size = 61 for othello\n",
    "    mconf = GPTConfig(61, 59, n_layer=8, n_head=8, n_embd=512)\n",
    "    model = GPT(mconf)\n",
    "    load_res = model.load_state_dict(torch.load(f\"./ckpts/{ckpt}.ckpt\"))\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "        model = model.to(device)\n",
    "        return model, device\n",
    "    else:\n",
    "        print(\"NO GPU FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if model prediction is legal for each node in given game\n",
    "# expects dataset has already been loaded and model is on GPU\n",
    "def check_legal(model, device, train_dataset, game):\n",
    "    total_nodes = 0\n",
    "    success_nodes = 0\n",
    "\n",
    "    len_whole_game = len(game)\n",
    "    for len_partial_game in range(1, len_whole_game):\n",
    "        total_nodes += 1\n",
    "        context = game[:len_partial_game]\n",
    "        x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None, ...].to(device)\n",
    "        y = sample(model, x, 1, temperature=1.0)\n",
    "        # taking top-1 prediction\n",
    "        completion = [train_dataset.itos[int(i)] for i in y[0] if i != -1]\n",
    "        try:\n",
    "            OthelloBoardState().update(completion)\n",
    "        except Exception:\n",
    "            # print(completion)\n",
    "            pass\n",
    "        else:\n",
    "            success_nodes += 1\n",
    "    \n",
    "    return total_nodes, success_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default data root is othello_synthetic\n",
    "def validate_with_dataset(model, device, data_root=None, n_games=1000):\n",
    "    # find to load in first n games, because the first ~1 million othello_synthetic games are test set for unbiased model\n",
    "    val_games = Othello(data_root=data_root, n_games=n_games, test_split=1, deduplicate=False)\n",
    "    char = CharDataset(val_games.val)\n",
    "\n",
    "    total_nodes = 0\n",
    "    success_nodes = 0\n",
    "\n",
    "    def progress_report():\n",
    "        return f\"{success_nodes/total_nodes*100:.4f}% pass rate: {success_nodes}/{total_nodes} among all searched nodes\"\n",
    "    \n",
    "    bar = tqdm(val_games.val[:n_games])\n",
    "    for game in bar:\n",
    "        tn, sn = check_legal(model, device, char, game)\n",
    "        total_nodes += tn\n",
    "        success_nodes += sn\n",
    "        bar.set_description(progress_report())\n",
    "    print(progress_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_from_checkpoint(ckpt, data_root=None, n_games=1000):\n",
    "    model, device = load_othello_model(ckpt)\n",
    "    validate_with_dataset(model, device, data_root=data_root, n_games=n_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mem Used: 4.062 GB:   0%|          | 0/50 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 from 1 files\n",
      "Using 0 for training, 1000 for validation\n",
      "Dataset created has 1000 sequences, 61 unique words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99.9474% pass rate: 58909/58940 among all searched nodes: 100%|██████████| 1000/1000 [06:48<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9474% pass rate: 58909/58940 among all searched nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validate_from_checkpoint(\"bias/finetune_bias80_e5\", data_root=\"othello_synthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"bias/TLcontrol\", \"bias/TLbias50\", \"bias/TLbias80\", \"bias/TLbias95\"]:\n",
    "    for dr in [\"synthetic\", \"TLbias50\", \"TLbias80\", \"TLbias95\"]:\n",
    "        print(f\"======== ckpt: {c} | data: {dr} =========\")\n",
    "        validate_from_checkpoint(c, data_root=f\"othello_{dr}\", n_games=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines for legal move accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eight directions\n",
    "eights = [[-1, 0], [-1, 1], [0, 1], [1, 1], [1, 0], [1, -1], [0, -1], [-1, -1]]\n",
    "# adds up empty spaces in current board state that have an adjacent occupied square\n",
    "def check_adj(ob):\n",
    "    total = 0\n",
    "    occupied = ob.get_occupied()\n",
    "    for i in range(64):\n",
    "        r, c = i // 8, i % 8\n",
    "        adj = False\n",
    "        if occupied[i]:\n",
    "            continue\n",
    "        for dir in eights:\n",
    "            test_r, test_c = r + dir[0], c + dir[1]\n",
    "            if test_r not in range(8) or test_c not in range(8):\n",
    "                continue\n",
    "            if occupied[test_r * 8 + test_c]:\n",
    "                adj = True\n",
    "                break\n",
    "        total += 1 if adj else 0\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "othello = Othello(data_root=\"othello_synthetic\", n_games=1000, test_split=0, deduplicate=False)\n",
    "baselines = [\n",
    "    0, # full random\n",
    "    0, # no-repeats random\n",
    "    0  # only adjacent\n",
    "]\n",
    "legal = 0\n",
    "bar = tqdm(othello)\n",
    "for seq in bar:\n",
    "    ob = OthelloBoardState()\n",
    "    for i, move in enumerate(seq):\n",
    "        baselines[0] += 60\n",
    "        baselines[1] += 60 - i\n",
    "        baselines[2] += check_adj(ob)        \n",
    "        legal += len(ob.get_valid_moves())\n",
    "        ob.update([move])\n",
    "    bar.set_description(desc=f\"{legal}/{baselines[0]}, {legal/baselines[0]:.4f}\")\n",
    "\n",
    "for b in baselines:\n",
    "    print(legal/b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
